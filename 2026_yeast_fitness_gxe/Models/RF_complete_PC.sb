#!/bin/bash --login 
#SBATCH --array=1-35
#SBATCH --time=1:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=5
#SBATCH --mem-per-cpu=3G
#SBATCH --job-name rf_PCs_sklearn
#SBATCH --output=%x_%j.out

cd /mnt/research/glbrc_group/shiulab/kenia/yeast_project/SNP_yeast_RF_results/baseline
TRAITS=($(head -n 1 /mnt/research/glbrc_group/shiulab/kenia/Shiu_Lab/Project/Data/Peter_2018/pheno.csv | sed 's/,/ /g'))
PIPE=/mnt/research/glbrc_group/shiulab/kenia/Shiu_Lab/Project/External_software/ML-Pipeline
DATA=/mnt/research/glbrc_group/shiulab/kenia/Shiu_Lab/Project/Data/Peter_2018

echo "${SLURM_ARRAY_TASK_ID} ; ${TRAITS[${SLURM_ARRAY_TASK_ID}]}"

# BASED ON TOP 5 PCs (64456 x 5)
module purge
conda activate /mnt/home/seguraab/miniconda3/envs/ml-pipeline
python ${PIPE}/ML_regression.py -df ${DATA}/PCs_first5.csv \
    -df2 ${DATA}/pheno.csv \
    -y_name ${TRAITS[${SLURM_ARRAY_TASK_ID}]} \
    -test ${DATA}/Test.txt \
    -sep , -alg RF -n_jobs 12 -n 20 -cv_num 5 \
    -save ${TRAITS[${SLURM_ARRAY_TASK_ID}]}_PCs_sklearn -plots t
conda deactivate
scontrol show job $SLURM_JOB_ID
